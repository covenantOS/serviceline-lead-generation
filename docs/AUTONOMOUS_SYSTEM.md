# ServiceLine Autonomous Lead Generation System

## ðŸ¤– Overview

The ServiceLine system is designed to run **completely autonomously** with minimal user intervention. Once configured, the system will:

1. **Automatically scrape** new leads daily
2. **Score leads** using AI-powered algorithms
3. **Send targeted emails** to hot leads (score â‰¥ 80)
4. **Follow up automatically** with intelligent sequences
5. **Track engagement** and update lead status
6. **Self-monitor** and alert on issues

---

## ðŸ—ï¸ Architecture

### Core Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Cron Scheduler                            â”‚
â”‚  (Triggers jobs at scheduled times)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Bull Job Queues (Redis)                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚Scraping â”‚ â”‚ Scoring â”‚ â”‚  Email  â”‚ â”‚  Follow-up â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Queue Workers                             â”‚
â”‚  (Process jobs asynchronously with retry logic)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Business Logic                            â”‚
â”‚  â€¢ Web Scrapers    â€¢ Lead Scorer    â€¢ Email Service          â”‚
â”‚  â€¢ Supabase DB     â€¢ Analytics      â€¢ Monitoring             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš™ï¸ Setup Instructions

### 1. Prerequisites

Install required services:

```bash
# Install Redis (required for job queues)
# macOS
brew install redis
brew services start redis

# Ubuntu/Debian
sudo apt-get install redis-server
sudo systemctl start redis

# Windows (using WSL or Memurai)
https://memurai.com/
```

### 2. Install Dependencies

```bash
npm install
```

### 3. Configure Environment

```bash
# Copy example configuration
cp config/.env.example config/.env

# Edit config/.env with your settings
nano config/.env
```

**Required Configuration:**

```env
# Enable autonomous operation
ENABLE_CRON=true

# Redis (for queues)
REDIS_HOST=localhost
REDIS_PORT=6379

# Supabase
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_SERVICE_KEY=your_service_key

# Email service (choose one)
EMAIL_SERVICE=sendgrid
SENDGRID_API_KEY=your_key

# Automation thresholds
AUTO_EMAIL_THRESHOLD=80
TARGET_INDUSTRIES=HVAC,PLUMBING,ROOFING,ELECTRICAL
TARGET_LOCATIONS=Phoenix AZ,Los Angeles CA,Houston TX
```

### 4. Setup Database

Run the database schema in Supabase SQL Editor:

```bash
# Copy schema from database/schema.sql and execute in Supabase
```

### 5. Start the System

```bash
# Terminal 1: Start API server (with cron jobs)
npm start

# Terminal 2: Start queue workers
npm run worker
```

---

## ðŸ“… Automated Schedules

### Daily Operations

| Time | Task | Description |
|------|------|-------------|
| **2:00 AM** | Daily Scraping | Scrapes 25 leads per industry from target locations |
| **Every 4 hours** | Lead Scoring | Scores all unscored leads automatically |
| **9:00 AM (Weekdays)** | Hot Leads Campaign | Sends intro emails to hot leads (score â‰¥80) |
| **10:00 AM** | Follow-up Check | Queues follow-ups for contacted leads |
| **12:00 AM** | Queue Cleanup | Cleans old completed jobs |
| **Every hour** | Health Check | Monitors system health |

### Weekly Operations

| Day | Time | Task |
|-----|------|------|
| **Sunday** | 3:00 AM | Deep Scraping (100 leads/industry, 8+ cities) |

---

## ðŸ”„ Automated Workflows

### 1. Lead Discovery â†’ Scoring â†’ Campaign

```
New Lead Scraped
       â†“
Automatically Queued for Scoring
       â†“
Score Calculated (0-100)
       â†“
Score â‰¥ 80? â†’ YES â†’ Auto-queue intro email (5 min delay)
       â†“              â†“
       NO            Schedule follow-up sequence
       â†“              â€¢ Day 3: Follow-up 1
Stored in DB         â€¢ Day 7: Follow-up 2
                     â€¢ Day 14: Case study
```

### 2. Follow-up Sequence

```
Intro Email Sent
       â†“
Wait 3 days â†’ Email opened? â†’ YES â†’ Mark as "Qualified"
       â†“                              Cancel automated follow-ups
       NO
       â†“
Send Follow-up #1
       â†“
Wait 4 days â†’ Email clicked? â†’ YES â†’ Mark as "Qualified"
       â†“                               Alert sales team
       NO
       â†“
Send Follow-up #2
       â†“
Wait 7 days â†’ No engagement? â†’ Mark as "Lost"
       â†“
Send Case Study
```

### 3. Email Engagement Tracking

```
Email Delivered â†’ Webhook from SendGrid/Mailgun
       â†“
Email Opened â†’ Update lead status to "Qualified"
       â†“          Add +5 to engagement score
Email Clicked â†’ Update to "Qualified"
       â†“          Add +10 to engagement score
                  Cancel scheduled follow-ups
                  Queue enrichment job
```

---

## ðŸ“Š Monitoring Dashboard

### Health Check Endpoint

```bash
# Check system health
curl http://localhost:3000/health

# Response
{
  "status": "healthy",
  "components": {
    "database": { "status": "healthy", "responseTime": 45 },
    "redis": { "status": "healthy", "responseTime": 12 },
    "queues": { "status": "healthy" },
    "scraping": { "status": "healthy", "lastJob": "2026-01-29T02:00:00Z" }
  }
}
```

### Monitoring Dashboard

Access detailed metrics (requires authentication):

```bash
# Get system metrics
GET /api/monitoring/metrics

# Get queue statistics
GET /api/monitoring/queues

# Get comprehensive dashboard
GET /api/monitoring/dashboard
```

---

## ðŸ”§ Queue System

### Queue Types

1. **Scraping Queue**: Web scraping jobs
   - Timeout: 10 minutes
   - Retries: 2
   - Concurrency: 1

2. **Scoring Queue**: Lead scoring
   - Timeout: 1 minute
   - Retries: 3
   - Concurrency: 1

3. **Email Queue**: Email sending
   - Timeout: 30 seconds
   - Retries: 5
   - Concurrency: 5 (sends 5 emails in parallel)

4. **Campaign Queue**: Campaign execution
   - Timeout: 5 minutes
   - Retries: 3
   - Concurrency: 1

5. **Follow-up Queue**: Follow-up sequences
   - Timeout: 2 minutes
   - Retries: 3
   - Concurrency: 3

6. **Enrichment Queue**: Data enrichment
   - Timeout: 90 seconds
   - Retries: 3
   - Concurrency: 2

### Managing Queues

```bash
# View queue statistics via API
curl -H "Authorization: Bearer YOUR_TOKEN" \
  http://localhost:3000/api/monitoring/queues

# Pause a queue (admin only)
# Implement via admin panel or direct Redis commands
```

---

## ðŸŽ¯ Lead Scoring Logic

The system uses **inverse scoring** - poor digital presence = high score = more opportunity.

### Scoring Components

| Component | Weight | What's Checked |
|-----------|--------|----------------|
| Website Quality | 25% | SSL, mobile, speed, analytics, meta tags |
| SEO Ranking | 20% | Google indexing, keyword rankings |
| Ad Presence | 15% | Google Ads, Facebook Ads |
| Review Score | 15% | Rating, review count, response rate |
| Social Presence | 10% | Facebook, Instagram, LinkedIn, etc. |
| Company Size | 10% | Employees, review volume |
| Market Competition | 5% | Location competitiveness |

### Lead Tiers

- **Hot Lead (80-100)**: Auto-email triggered immediately
- **Warm Lead (60-79)**: Added to campaign pool
- **Cold Lead (40-59)**: Monitored, no auto-contact
- **Low Priority (0-39)**: Stored for future campaigns

---

## ðŸ“§ Email Webhooks

### SendGrid Webhook Setup

1. Go to SendGrid Settings â†’ Mail Settings â†’ Event Webhook
2. Set URL: `https://your-domain.com/api/webhooks/sendgrid`
3. Enable events:
   - Delivered
   - Opened
   - Clicked
   - Bounced
   - Spam Report

### Mailgun Webhook Setup

1. Go to Mailgun â†’ Sending â†’ Webhooks
2. Add webhook: `https://your-domain.com/api/webhooks/mailgun`
3. Enable events:
   - delivered
   - opened
   - clicked
   - bounced
   - complained

---

## ðŸš¨ Error Handling & Recovery

### Automatic Retry Logic

- **Scraping failures**: 2 retries with exponential backoff
- **Email failures**: 5 retries (handles temporary issues)
- **Scoring failures**: 3 retries
- **Campaign failures**: 3 retries

### Failed Job Management

```bash
# View failed jobs in monitoring dashboard
GET /api/monitoring/queues

# Failed jobs are kept for 500 jobs (debugging)
# Completed jobs are kept for 100 jobs
```

### Health Monitoring

System automatically checks every hour:
- Database connectivity
- Redis connectivity
- Queue health (failed jobs, stalls)
- Recent scraping success
- Email delivery rate
- System resources (memory, uptime)

**Status Levels:**
- `healthy`: All systems operational
- `degraded`: Some issues detected, system still functional
- `critical`: Major issues, requires attention

---

## ðŸ” Security Considerations

### Webhook Security

Webhooks are public endpoints. For production:

1. **Validate webhook signatures** (SendGrid/Mailgun provide signing)
2. **Use HTTPS** for all webhook URLs
3. **Implement IP whitelisting** if provider supports it
4. **Monitor webhook failures** for potential attacks

### Queue Security

- Redis should not be exposed to public internet
- Use Redis password authentication
- Implement rate limiting on job creation

---

## ðŸ“ˆ Scaling the System

### Horizontal Scaling

1. **Multiple Workers**: Run multiple `npm run worker` instances
2. **Load Balancer**: Put API servers behind load balancer
3. **Redis Cluster**: Use Redis cluster for high availability
4. **Database Read Replicas**: Scale Supabase with read replicas

### Performance Tuning

```env
# Increase email concurrency
# In queue-config.js, adjust email queue concurrency

# Adjust scraping limits
MAX_CONCURRENT_SCRAPES=10

# Tune worker concurrency
# Run more worker processes on different machines
```

---

## ðŸ› Troubleshooting

### Cron Jobs Not Running

```bash
# Check if cron is enabled
echo $ENABLE_CRON  # Should be 'true'

# Check logs
tail -f logs/app.log | grep "cron"

# Verify cron jobs are initialized
curl http://localhost:3000/health
```

### Queue Workers Not Processing

```bash
# Check if Redis is running
redis-cli ping  # Should return 'PONG'

# Check if workers are running
ps aux | grep queue-worker

# Restart workers
npm run worker
```

### Emails Not Sending

```bash
# Check email configuration
echo $EMAIL_SERVICE
echo $SENDGRID_API_KEY

# Check failed email jobs
GET /api/monitoring/queues
# Look at 'email' queue failed count

# Test email manually
POST /api/messages/send
```

### High Memory Usage

```bash
# Check system health
GET /api/monitoring/health

# Clean old queue jobs
# Runs automatically at midnight
# Or trigger manually via Redis CLI
```

---

## ðŸŽ“ Best Practices

### 1. Monitor Daily

- Check `/api/monitoring/dashboard` each morning
- Review failed jobs weekly
- Monitor email delivery rates

### 2. Adjust Thresholds

```env
# If too many auto-emails
AUTO_EMAIL_THRESHOLD=85

# If too few leads
MIN_SCORE_THRESHOLD=50
```

### 3. Review Templates

- Update email templates monthly
- A/B test subject lines
- Personalize based on industry

### 4. Database Maintenance

- Run `VACUUM` on PostgreSQL monthly
- Archive old leads (>6 months no activity)
- Back up database weekly

### 5. Security Updates

```bash
# Check for updates monthly
npm audit
npm update

# Update dependencies
npm install
```

---

## ðŸ“ž Support

For issues or questions:
- Check logs: `tail -f logs/app.log`
- Review queue stats: `GET /api/monitoring/queues`
- Check health: `GET /api/monitoring/health`
- Open GitHub issue

---

## ðŸš€ Quick Start Checklist

- [ ] Redis installed and running
- [ ] Environment variables configured
- [ ] Database schema deployed
- [ ] API server started (`npm start`)
- [ ] Workers started (`npm run worker`)
- [ ] `ENABLE_CRON=true` set
- [ ] Email webhooks configured
- [ ] Health check returns "healthy"
- [ ] First scraping job scheduled

**System is now running autonomously!** ðŸŽ‰
